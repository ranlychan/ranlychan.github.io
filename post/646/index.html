
<!DOCTYPE html>
<html>
  <head>
    
<meta charset="utf-8" >

<title>阅读笔记｜Attention Is All You Need | 蓝里小窝 | ranlychan&#39;s blog</title>
<meta name="description" content="笔耕不辍，汇涓成河。">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://ranlychan.github.io/favicon.ico?v=1735555799576">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://ranlychan.github.io/styles/main.css">



<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>



  </head>
  <body>
    <div id="app" class="main">
      <div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="https://ranlychan.github.io">
        <img class="avatar" src="https://ranlychan.github.io/images/avatar.png?v=1735555799576" alt="" width="32px" height="32px">
      </a>
      <a href="https://ranlychan.github.io">
        <h1 class="site-title">蓝里小窝 | ranlychan&#39;s blog</h1>
      </a>
    </div>
    <div class="right">
      <transition name="fade">
        <i class="icon" :class="{ 'icon-close-outline': menuVisible, 'icon-menu-outline': !menuVisible }" @click="menuVisible = !menuVisible"></i>
      </transition>
    </div>
  </div>
</div>

<transition name="fade">
  <div class="menu-container" style="display: none;" v-show="menuVisible">
    <div class="menu-list">
      
        
          <a href="/" class="menu purple-link">
            首页
          </a>
        
      
        
          <a href="/archives" class="menu purple-link">
            归档
          </a>
        
      
        
          <a href="/tags" class="menu purple-link">
            标签
          </a>
        
      
        
          <a href="/post/about" class="menu purple-link">
            关于
          </a>
        
      
    </div>
  </div>
</transition>


      <div class="content-container">
        <div class="post-detail">
          
          <h2 class="post-title">阅读笔记｜Attention Is All You Need</h2>
          <div class="post-info post-detail-info">
            <span><i class="icon-calendar-outline"></i> 2023-08-29</span>
            
              <span>
                <i class="icon-pricetags-outline"></i>
                
                  <a href="https://ranlychan.github.io/tag/nV5WC7Xwi-/">
                    计算机网络
                    
                  </a>
                
              </span>
            
          </div>
          <div class="post-content" v-pre>
            <p>[info] A. Vaswani et al., “Attention Is All You Need,” 2017, doi: 10.48550/ARXIV.1706.03762. [/info]</p>
<!--more-->
<h2 id="11-背景">1.1 背景</h2>
<ul>
<li>Transformer一开始主要针对NLP中的翻译任务</li>
<li>主流序列到序列模型通常基于复杂的RNN或CNN</li>
<li>RNN由于结构特性存在并行性差的问题，且长记忆差</li>
<li>CNN一次按一个窗口进行卷积，相距远的信息需要多重卷积才能融合</li>
<li>目前表现最好的模型包含编码器和解码器结构，并用注意力机制连接两者</li>
</ul>
<h2 id="12-主要贡献">1.2 主要贡献</h2>
<ul>
<li>提出Transformer，一个完全基于自注意力的新模型结构，区别于RNN或CNN</li>
<li>创新发展了注意力机制，提出多头注意力机制用于取代传统seq2seq中的递归层，兼具并行性同时有效建模了长距离依赖</li>
<li>将建模长距离依赖关系的操作的复杂度从RNN的线性和CNN的对数降至常数级别</li>
</ul>
<h2 id="13-模型结构">1.3 模型结构</h2>
<h3 id="整体架构">整体架构</h3>
<ul>
<li>采用编码器-解码器结构</li>
<li>编码器和解码器都是N个相同的层叠加</li>
</ul>
<h3 id="编码器">编码器</h3>
<ul>
<li>N=6个相同的层叠加</li>
<li>每层包含两部分</li>
<li>多头自注意力</li>
<li>全连接的前馈网络</li>
<li>每层后都有残差连接和层规范化</li>
</ul>
<h3 id="解码器">解码器</h3>
<ul>
<li>与编码器结构类似</li>
<li>在每个编码器层后面，添加一个多头自注意力用于编码器输出</li>
<li>自回归（前面的输出作为输入）</li>
<li>增加掩码机制，防止训练时由于自回归而提前看到后面的位置</li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li>BatchNorm：对于二维输入来说，对每列的同一特征进行normalization；三维输入中，依然是对于同一特征，不同batch和不同seq的数据进行normalization</li>
<li>LayerNorm：对于二维输入来说，对每行的样本进行normalization；三维输入中，则是对于同一个batch，不同feature和不同seq的数据进行normalizationBatchNorm：对于二维输入来说，对每列的同一特征进行normalization；三维输入中，依然是对于同一特征，不同batch和不同seq的数据进行normalization</li>
</ul>
<h3 id="位置编码">位置编码</h3>
<ul>
<li>因为没有RNN自带的时序结构，故输入添加正弦和余弦函数编码位置信息<br>
<img src="https://s2.loli.net/2023/09/05/6xC4ahIDHvGipsB.png" alt="" loading="lazy"></li>
</ul>
<h3 id="自注意力">自注意力</h3>
<ul>
<li>Scaled Dot-Product Attention</li>
<li>将查询、键、值打包成矩阵，做缩放点积注意力</li>
<li>用参数dk控制缩放</li>
<li>Multi-Head Attention</li>
<li>将输入进行多次线性映射，得到多个头部</li>
<li>在每个头部单独做点积注意力</li>
<li>将所有头部连接起来做线性映射</li>
<li>允许注意不同的子空间</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2023/09/05/yUFL5Ww62l3pCZE.jpg" alt="" loading="lazy"></figure>
<h2 id="14-实验">1.4 实验</h2>
<h3 id="训练">训练</h3>
<ul>
<li>标准机器翻译数据集</li>
<li>Adam优化器</li>
<li>标签平滑、残差dropout等正则化策略</li>
</ul>
<h3 id="结果">结果</h3>
<ul>
<li>英德和英法翻译任务表现再创新高</li>
<li>英语短语结构分析也取得良好结果</li>
</ul>
<h2 id="15-个人思考">1.5 个人思考</h2>
<ul>
<li>Transformer并不是只要attention就行，MLP和残差连接等缺一不可，只是相对以前的seq2seq没有了CNN或RNN。</li>
<li>Transformer的注意力机制没有像RNN那样在结构上去对序列做顺序上的建模，能处理更一般化的信息，能比CNN取得更好的效果，代价是需要更多数据和训练，模型越来越大。</li>
</ul>

          </div>
        </div>

        
          <div class="next-post">
            <a class="purple-link" href="https://ranlychan.github.io/post/641/">
              <h3 class="post-title">
                下一篇：阅读笔记｜Verifying and Monitoring IoTs Network Behavior Using MUD Profiles
              </h3>
            </a>
          </div>
          
      </div>

      

      <div class="site-footer">
  <div class="slogan">笔耕不辍，汇涓成河。</div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://ranlychan.github.io/atom.xml" target="_blank">RSS</a>
</div>


    </div>
    <script type="application/javascript">

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>




  </body>
</html>
